"""
Veo 3.1 Video Generation Service
Uses Google's Veo 3.1 API for AI video generation with dialogue and audio.
Documentation: https://ai.google.dev/gemini-api/docs/video

Features:
- Text-to-video generation with dialogue and sound effects
- Image-to-video (animate a generated character image)
- Reference images for character consistency
- First/Last frame control
- Video extension
"""

import time
import asyncio
from typing import Optional, Dict, Any, List
from pathlib import Path
from google import genai
from google.genai import types
from app.core.config import settings


class VeoVideoService:
    """Service for video generation using Google's Veo 3.1 API."""
    
    # Available models
    MODELS = {
        "veo-3.1": "veo-3.1-generate-preview",      # Best quality, native audio
        "veo-3.1-fast": "veo-3.1-fast-generate-preview",  # Faster, still good
        "veo-2": "veo-2.0-generate-001",            # Previous gen, no native audio
    }
    
    # Supported configurations
    ASPECT_RATIOS = ["16:9", "9:16"]
    RESOLUTIONS = ["720p", "1080p", "4k"]
    DURATIONS = [4, 5, 6, 8]  # seconds
    
    def __init__(self):
        self.client = genai.Client(api_key=settings.GEMINI_API_KEY)
        # Use Veo 3.1 by default for native audio support
        self.model_name = getattr(settings, 'VEO_MODEL', self.MODELS["veo-3.1"])
        self.poll_interval = getattr(settings, 'VEO_POLL_INTERVAL', 10)  # seconds
        self.max_wait_time = getattr(settings, 'VEO_MAX_WAIT_TIME', 360)  # 6 minutes max
        print(f"[VeoVideoService] Initialized with model: {self.model_name}")
    
    async def generate_from_text(
        self,
        prompt: str,
        aspect_ratio: str = "16:9",
        resolution: str = "720p",
        duration_seconds: int = 8,
        negative_prompt: str = None,
        person_generation: str = "allow_adult",
        seed: int = None,
    ) -> Dict[str, Any]:
        """
        Generate a video from a text prompt.
        
        Args:
            prompt: The text prompt describing the video scene, including dialogue
            aspect_ratio: "16:9" (landscape) or "9:16" (portrait)
            resolution: "720p", "1080p", or "4k"
            duration_seconds: Video duration (4, 5, 6, or 8 seconds)
            negative_prompt: Things to avoid in the video
            person_generation: "allow_all", "allow_adult", or "dont_allow"
            seed: Optional seed for slightly more deterministic results
            
        Returns:
            Dict with video_bytes, video_url, duration, and metadata
        """
        try:
            print(f"[Veo] Generating video from text...")
            print(f"[Veo] Prompt: {prompt[:100]}...")
            print(f"[Veo] Config: {aspect_ratio}, {resolution}, {duration_seconds}s")
            
            # Build generation config
            config = {
                "aspect_ratio": aspect_ratio,
                "resolution": resolution,
                "duration_seconds": str(duration_seconds),
                "person_generation": person_generation,
            }
            
            if negative_prompt:
                config["negative_prompt"] = negative_prompt
            if seed is not None:
                config["seed"] = seed
            
            # Start the async video generation operation
            operation = self.client.models.generate_videos(
                model=self.model_name,
                prompt=prompt,
                config=types.GenerateVideosConfig(**config),
            )
            
            print(f"[Veo] Operation started, polling for completion...")
            
            # Poll until complete
            video_result = await self._poll_operation(operation)
            
            return video_result
            
        except Exception as e:
            print(f"[Veo] [ERROR] Text-to-video failed: {str(e)}")
            raise Exception(f"Veo Video Generation Failed: {str(e)}")
    
    async def generate_from_image(
        self,
        prompt: str,
        image_bytes: bytes,
        aspect_ratio: str = "16:9",
        resolution: str = "720p",
        duration_seconds: int = 8,
        negative_prompt: str = None,
        person_generation: str = "allow_adult",
    ) -> Dict[str, Any]:
        """
        Generate a video using an image as the starting frame.
        
        This is ideal for animating character images generated by Gemini.
        
        Args:
            prompt: The text prompt describing the video action
            image_bytes: The image to use as the first frame (from GeminiImageService)
            aspect_ratio: "16:9" or "9:16"
            resolution: "720p", "1080p", or "4k"
            duration_seconds: Video duration
            negative_prompt: Things to avoid
            person_generation: Person generation policy
            
        Returns:
            Dict with video_bytes, video_url, duration, and metadata
        """
        try:
            print(f"[Veo] Generating video from image...")
            print(f"[Veo] Prompt: {prompt[:100]}...")
            print(f"[Veo] Image size: {len(image_bytes)} bytes")
            
            # Create image part
            image_part = types.Part.from_bytes(
                data=image_bytes,
                mime_type="image/jpeg"
            )
            
            # Build config
            config = {
                "aspect_ratio": aspect_ratio,
                "resolution": resolution,
                "duration_seconds": str(duration_seconds),
                "person_generation": person_generation,
            }
            
            if negative_prompt:
                config["negative_prompt"] = negative_prompt
            
            # Start the async video generation with image
            operation = self.client.models.generate_videos(
                model=self.model_name,
                prompt=prompt,
                image=image_part,
                config=types.GenerateVideosConfig(**config),
            )
            
            print(f"[Veo] Operation started with image input...")
            
            # Poll until complete
            video_result = await self._poll_operation(operation)
            
            return video_result
            
        except Exception as e:
            print(f"[Veo] [ERROR] Image-to-video failed: {str(e)}")
            raise Exception(f"Veo Image-to-Video Failed: {str(e)}")
    
    async def generate_with_reference_images(
        self,
        prompt: str,
        reference_images: List[bytes],
        aspect_ratio: str = "16:9",
        resolution: str = "720p",
        duration_seconds: int = 8,
        negative_prompt: str = None,
        person_generation: str = "allow_adult",
    ) -> Dict[str, Any]:
        """
        Generate a video using reference images for style/character consistency.
        
        Args:
            prompt: The text prompt for the video
            reference_images: List of image bytes for style/character reference
            aspect_ratio: "16:9" or "9:16"
            resolution: Video resolution
            duration_seconds: Video duration
            negative_prompt: Things to avoid
            person_generation: Person generation policy
            
        Returns:
            Dict with video_bytes, video_url, duration, and metadata
        """
        import base64
        
        try:
            print(f"[Veo] Generating video with {len(reference_images)} reference images...")
            
            # Build reference image list using VideoGenerationReferenceImage
            # The image field expects an Image object with base64 encoded data
            ref_images = []
            for idx, img_bytes in enumerate(reference_images[:3]):  # Max 3 references
                # Create Image object with base64 encoded bytes
                img_obj = types.Image(
                    image_bytes=img_bytes,
                    mime_type="image/jpeg"
                )
                ref_images.append(
                    types.VideoGenerationReferenceImage(
                        image=img_obj,
                        reference_type="asset"
                    )
                )
            
            # Build config with reference images inside
            # Note: duration_seconds MUST be "8" when using reference images
            config = types.GenerateVideosConfig(
                aspect_ratio=aspect_ratio,
                person_generation=person_generation,
                reference_images=ref_images,
            )
            
            if negative_prompt:
                config.negative_prompt = negative_prompt
            
            # Start generation with reference images
            operation = self.client.models.generate_videos(
                model=self.model_name,
                prompt=prompt,
                config=config,
            )
            
            print(f"[Veo] Operation started with reference images...")
            
            video_result = await self._poll_operation(operation)
            
            return video_result
            
        except Exception as e:
            print(f"[Veo] [ERROR] Reference image video failed: {str(e)}")
            raise Exception(f"Veo Reference Video Failed: {str(e)}")
    
    async def generate_with_frames(
        self,
        prompt: str,
        first_frame_bytes: bytes,
        last_frame_bytes: bytes,
        aspect_ratio: str = "16:9",
        resolution: str = "720p",
        duration_seconds: int = 8,
        person_generation: str = "allow_adult",
    ) -> Dict[str, Any]:
        """
        Generate a video with controlled first and last frames.
        
        This is useful for creating transitions between two character poses.
        
        Args:
            prompt: The text prompt for the transition
            first_frame_bytes: Starting image
            last_frame_bytes: Ending image
            aspect_ratio: "16:9" or "9:16"
            resolution: Video resolution
            duration_seconds: Video duration
            person_generation: Person generation policy
            
        Returns:
            Dict with video_bytes, video_url, duration, and metadata
        """
        try:
            print(f"[Veo] Generating video with first+last frame control...")
            
            # Create image parts
            first_frame_part = types.Part.from_bytes(data=first_frame_bytes, mime_type="image/jpeg")
            last_frame_part = types.Part.from_bytes(data=last_frame_bytes, mime_type="image/jpeg")
            
            # Build config
            config = {
                "aspect_ratio": aspect_ratio,
                "resolution": resolution,
                "duration_seconds": str(duration_seconds),
                "person_generation": person_generation,
                "last_frame": last_frame_part,
            }
            
            # Start generation with both frames
            operation = self.client.models.generate_videos(
                model=self.model_name,
                prompt=prompt,
                image=first_frame_part,
                config=types.GenerateVideosConfig(**config),
            )
            
            print(f"[Veo] Operation started with first+last frames...")
            
            video_result = await self._poll_operation(operation)
            
            return video_result
            
        except Exception as e:
            print(f"[Veo] [ERROR] Frame-controlled video failed: {str(e)}")
            raise Exception(f"Veo Frame Video Failed: {str(e)}")
    
    async def extend_video(
        self,
        video_bytes: bytes,
        prompt: str = None,
        duration_seconds: int = 8,
    ) -> Dict[str, Any]:
        """
        Extend an existing Veo-generated video.
        
        Args:
            video_bytes: The original video bytes
            prompt: Optional prompt to guide the extension
            duration_seconds: Duration of the extension
            
        Returns:
            Dict with extended video_bytes and metadata
        """
        try:
            print(f"[Veo] Extending video...")
            
            # Create video part
            video_part = types.Part.from_bytes(data=video_bytes, mime_type="video/mp4")
            
            # Build config
            config = {
                "duration_seconds": str(duration_seconds),
            }
            
            # Start extension operation
            operation = self.client.models.generate_videos(
                model=self.model_name,
                prompt=prompt or "Continue the video naturally",
                video=video_part,
                config=types.GenerateVideosConfig(**config),
            )
            
            print(f"[Veo] Extension operation started...")
            
            video_result = await self._poll_operation(operation)
            
            return video_result
            
        except Exception as e:
            print(f"[Veo] [ERROR] Video extension failed: {str(e)}")
            raise Exception(f"Veo Video Extension Failed: {str(e)}")
    
    async def _poll_operation(self, operation) -> Dict[str, Any]:
        """
        Poll an async operation until it completes or times out.
        
        Args:
            operation: The operation object from generate_video
            
        Returns:
            Dict with video_bytes, video_url, duration, and metadata
        """
        start_time = time.time()
        
        try:
            # Poll until operation completes
            while True:
                elapsed = time.time() - start_time
                
                if elapsed > self.max_wait_time:
                    raise Exception(f"Video generation timed out after {self.max_wait_time}s")
                
                # Refresh operation status
                try:
                    operation = self.client.operations.get(name=operation.name)
                except Exception as e:
                    print(f"[Veo] [WARNING] Failed to get operation status: {e}")
                    await asyncio.sleep(self.poll_interval)
                    continue
                
                # Check if done
                is_done = False
                if hasattr(operation, 'done'):
                    is_done = operation.done
                elif hasattr(operation, 'metadata') and hasattr(operation.metadata, 'state'):
                    is_done = operation.metadata.state in ['SUCCEEDED', 'FAILED', 'CANCELLED']
                else:
                    # Fallback: check for response
                    is_done = hasattr(operation, 'response') and operation.response is not None
                
                if is_done:
                    break
                
                print(f"[Veo] Waiting for video... ({elapsed:.0f}s elapsed)")
                await asyncio.sleep(self.poll_interval)
            
            elapsed = time.time() - start_time
            print(f"[Veo] [OK] Operation completed in {elapsed:.1f}s")
            
            # Check for errors
            if hasattr(operation, 'error') and operation.error:
                error_msg = str(operation.error)
                print(f"[Veo] [ERROR] Operation failed: {error_msg}")
                raise Exception(f"Veo operation failed: {error_msg}")
            
            # Extract the generated video
            if not hasattr(operation, 'response') or not operation.response:
                raise Exception("No response in completed operation")
            
            if not hasattr(operation.response, 'generated_videos') or not operation.response.generated_videos:
                raise Exception("No videos in operation response")
            
            generated_video = operation.response.generated_videos[0]
            
            # Download the video bytes
            print(f"[Veo] Downloading video from URI...")
            
            video_bytes = None
            video_uri = None
            
            # Try different methods to get video content
            if hasattr(generated_video, 'video'):
                video_file = generated_video.video
                
                # Method 1: Direct download
                if hasattr(video_file, 'uri'):
                    video_uri = video_file.uri
                    print(f"[Veo] Video URI: {video_uri}")
                    
                    try:
                        # Download using the files API
                        downloaded_file = self.client.files.get(name=video_file.name)
                        if hasattr(downloaded_file, 'read'):
                            video_bytes = downloaded_file.read()
                        elif hasattr(downloaded_file, 'data'):
                            video_bytes = downloaded_file.data
                        print(f"[Veo] Downloaded {len(video_bytes)} bytes")
                    except Exception as e:
                        print(f"[Veo] [WARNING] Download via files API failed: {e}")
                
                # Method 2: Direct read if file is already opened
                if not video_bytes and hasattr(video_file, 'read'):
                    video_bytes = video_file.read()
                    print(f"[Veo] Read {len(video_bytes)} bytes from file object")
            
            if not video_bytes:
                raise Exception("Failed to download video bytes from operation")
            
            return {
                "video_bytes": video_bytes,
                "video_url": video_uri,
                "generation_time_seconds": elapsed,
                "model": self.model_name,
                "status": "success"
            }
            
        except Exception as e:
            print(f"[Veo] [ERROR] Operation polling failed: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    async def save_video(
        self,
        video_result: Dict[str, Any],
        output_path: str
    ) -> str:
        """
        Save a generated video to a file.
        
        Args:
            video_result: Result dict from generate_* methods
            output_path: Path to save the video (e.g., "output.mp4")
            
        Returns:
            Absolute path to saved file
        """
        try:
            path = Path(output_path)
            path.parent.mkdir(parents=True, exist_ok=True)
            
            if video_result.get("video_bytes"):
                with open(path, "wb") as f:
                    f.write(video_result["video_bytes"])
                print(f"[Veo] [OK] Video saved to {path}")
                return str(path.absolute())
            else:
                raise Exception("No video bytes in result")
                
        except Exception as e:
            print(f"[Veo] [ERROR] Failed to save video: {str(e)}")
            raise


class VeoPromptOptimizer:
    """Helper class for optimizing prompts for Veo video generation."""
    
    @staticmethod
    def create_dialogue_prompt(
        scene_description: str,
        dialogue: List[Dict[str, str]],
        sound_effects: List[str] = None,
        camera_movement: str = None,
    ) -> str:
        """
        Create an optimized prompt for dialogue-heavy scenes.
        
        Args:
            scene_description: Visual description of the scene
            dialogue: List of {"speaker": "name", "line": "text"} dicts
            sound_effects: Optional list of sound effects to include
            camera_movement: Optional camera direction (e.g., "slow zoom in")
            
        Returns:
            Optimized prompt string
        """
        prompt_parts = [scene_description]
        
        # Add dialogue
        for d in dialogue:
            speaker = d.get("speaker", "A person")
            line = d.get("line", "")
            emotion = d.get("emotion", "")
            
            if emotion:
                prompt_parts.append(f"{speaker} says {emotion}, '{line}'")
            else:
                prompt_parts.append(f"{speaker} says, '{line}'")
        
        # Add sound effects
        if sound_effects:
            prompt_parts.append(f"Sound effects: {', '.join(sound_effects)}")
        
        # Add camera movement
        if camera_movement:
            prompt_parts.append(f"Camera: {camera_movement}")
        
        return " ".join(prompt_parts)
    
    @staticmethod
    def create_cinematic_prompt(
        scene_description: str,
        style: str = "cinematic",
        lighting: str = None,
        color_grade: str = None,
    ) -> str:
        """
        Create an optimized prompt for cinematic visuals.
        
        Args:
            scene_description: The scene to generate
            style: Visual style (e.g., "cinematic", "documentary", "animation")
            lighting: Lighting description
            color_grade: Color grading style
            
        Returns:
            Optimized prompt string
        """
        prompt_parts = [scene_description]
        
        style_tags = []
        if style:
            style_tags.append(style)
        if lighting:
            style_tags.append(f"{lighting} lighting")
        if color_grade:
            style_tags.append(f"{color_grade} color grade")
        
        if style_tags:
            prompt_parts.append(f"Style: {', '.join(style_tags)}")
        
        return " ".join(prompt_parts)
